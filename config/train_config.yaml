model_name: mistralai/Mistral-7B-Instruct-v0.2
max_seq_length: 2048
load_in_4bit: true
r: 16
target_modules: 
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj
lora_alpha: 16
lora_dropout: 0.05
bias: none
use_gradient_checkpointing: true
random_state: 42
per_device_train_batch_size: 32
gradient_accumulation_steps: 4
warmup_steps: 100
num_train_epochs: 3
max_steps: -1
learning_rate: 2.0e-4
logging_steps: 25
weight_decay: 0.01
dataset_num_proc: 4
packing: true
output_dir: ./outputs/trained_model
push_to_hub: false
hf_username: your_username
trained_model_name: your_model_name
dataset_file: ./datasets/your_dataset.json